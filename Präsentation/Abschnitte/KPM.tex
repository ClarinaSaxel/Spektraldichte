\begin{frame}
    \frametitle{Einführung}
    \begin{itemize}
        \item Zwei Klassen, KPM und Lanczos-Spektrokopieverfahren
        \item Zwei Methoden sind äquivalent zur KPM
        \item Die andere Klasse basiert auf der partiellen Tridiagonalisierung
        \item Zwei Methoden, Gaußscher und Lorentzscher Weichzeichnung (Regularisierung)
        \item Alle Methoden benutzen eine stochastische Sampling-Methode, die auf folgendem Resultat basiert:
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Theorem}
    Sei $A \in \R^{n \times n}$ mit Spektralzerlegung $A = \sum_{j = 1}^n \lambda_j u_j u_j^T$ wobei $u_iu_j^T = \delta_{ij}$.\\
    Sei außerdem $v \in \R^n$ mit $v = \sum_{j=1}^n \beta_j u_j$\\
    Gilt $v_i \sim \mathcal{N}(0, 1)$ i.i.d. für die Komponenten $\{v_i\}_{i = 1}^n$ von $v$, also $\E[v] = 0$ und $\E[vv^T] = \1_n$, dann
    $$\E[\beta_i \beta_j] = \delta_{ij}, \quad i,j \in \N_1^n$$
\end{frame}

\begin{frame}
    \frametitle{Beweisskizze}
    \begin{proof}[Beweis Theorem 1]
        Für $U = \left[u_1, u_2, \dots, u_n\right]$ und $\beta = \left(\beta_1, \beta_2, \dots, \beta_n\right)^T$ gilt $v = U \beta$\\
        Daher folgt, dass
        $$\E[v] = \E[U\beta] = U\E[\beta] = 0 \implies \E[\beta] = 0$$
        Weiterhin gilt, dass
        $$\E[vv^T] = \E[(U\beta)(U\beta)^T] = \E[U\beta \beta^TU^T] = U \E[\beta \beta^T]U^T = \1_n$$
        woraus folgt, dass $\E[\beta \beta^T] = \1_n$
    \end{proof}
\end{frame}

\begin{frame}
    \frametitle{Resultat}
    Sei $f(A)$ eine Matrixfunktion. Dann haben wir 
    \begin{align*}
        \E\left[v^Tf(A)v\right] & = \E\left[(U\beta)^Tf(U\Lambda U^T)(U\beta)\right]\\
        & = \E\left[\beta^TU^TUf(\Lambda)U^TU\beta\right]\\
        & = \E\left[\beta^Tf(\Lambda)\beta\right]\\
        & = \E\left[\sum_{j = 1}^n \beta_j^2 f(\lambda_j) \right]\\
        & = \sum_{j = 1}^n f(\lambda_j) \E\left[ \beta_j^2 \right]\\
        & = \sum_{j = 1}^n f(\lambda_j)
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Tschebyschev-Polynome}
    Mit Hilfe der trigonometrischen Funktionen können die Tschebyschev wie folgt ausgedrückt werden:
    \[ T_k(t) =
    \begin{cases}
        \cos(k \arccos(t))            & \quad \text{für } k \in [-1, 1]\\
        \cosh(k \arcosh(t))           & \quad \text{für } k > 1\\
        (-1)^k \cosh(k \arcosh(-t))   & \quad \text{für } k < -1
    \end{cases}
    \]
    Es gilt außerdem die Rekursionsformel
    $$T_{k + 1}(t) = 2tT_k(t) - T_{k - 1}(t)$$
\end{frame}

\begin{frame}
    \frametitle{Transformation der Eigenwerte}
    Wir benutzen im Folgenden $T_k(t) = \cos(k \arccos(t))$ um die Dirac-Dichte zu erweitern.\\
    Wir müssen uns daher auf Matrizen beschränken, deren Eigenwerte im Intervall $[-1, 1]$ liegen.\\
    Seien daher $\lambda_{us}$ und $\lambda_{os}$ jeweils die untere bzw. obere Schranke für die Eigenwerte von $A$.
    Definiere
    $$c := \frac{\lambda_{us} + \lambda_{os}}{2} \quad \text{und} \quad d := \frac{\lambda_{os} - \lambda_{us}}{2}$$
    Dann ist $B = \frac{A - c*\1_n}{d}$ eine Matrix mit Eigenwerten im Intervall $[-1, 1]$
\end{frame}

\begin{frame}
    \frametitle{Erweiterung der Delta-Distribution}
    Berechne zunächst
    $$\hat{\phi}(t) = \sqrt{1 - t^2} \phi(t) = \sqrt{1 - t^2} \times \frac{1}{n} \sum_{j = 1}^n \delta(t - \lambda_j)$$
    Nun erweitern wir
    $$\hat{\phi}(t) = \sum_{k = 0}^{\infty} \mu_k T_k(t)$$
    im Sinne von
    $$\int \limits_{-1}^1 \hat{\phi}(t) g(t) \dt = \int \limits_{-1}^1 \sum_{k = 0}^{\infty} \mu_k T_k(t) g(t) \dt$$
    für $g \in \SR$
\end{frame}

\begin{frame}
    \frametitle{Momentenmethode}
    \begin{align*}
        \mu_k &= \frac{2 - \delta_{k0}}{\pi} \int\limits_{-1}^1 \frac{1}{\sqrt{1 - t^2}} T_k(t) \hat\phi(t) \dt\\
        &= \frac{2 - \delta_{k0}}{\pi} \int\limits_{-1}^1 \frac{1}{\sqrt{1 - t^2}} T_k(t) \sqrt{1 - t^2} \phi(t) \dt\\
        &= \frac{2 - \delta_{k0}}{n \pi} \sum_{j = 1}^n T_k(\lambda_j)\\
        &= \frac{2 - \delta_{k0}}{n \pi} \Spur(T_k(A))
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Weitere Parameter}
    Es folgt, dass
    $$\zeta_k = \frac{1}{n_{vec}} \sum_{l = 1}^{n_{vec}} \left( v_0^{(l)} \right)^T T_k(A) v_0^{(l)}$$
    ein guter Schätzer für $\Spur(T_k(A))$ ist und damit
    $$\mu_k \approx \frac{2 - \delta_{k0}}{n \pi} \zeta_k$$
\end{frame}

\begin{frame}
    \frametitle{Berechnung der $\zeta_k$}
    Sei im Folgenden $v_0 \equiv v_0^{(l)}$
    Berechne nun
    $$T_{k + 1}(A)v_0 = 2 A T_k(A) v_0 - T_{k - 1}(A) v_0$$
    Für $v_k \equiv T_k(A)v_0$ gilt also, dass
    $$v_{k + 1} = 2 A v_k - v_{k - 1}$$
\end{frame}

\begin{frame}
    \frametitle{Definition $\tilde{\phi}_M(t)$}
    Sobald die $\{ \mu_k \}$ bestimmt sind, wäre die Erweiterung theoretisch durch
    $$\phi(t) = \frac{1}{\sqrt{1 - t^2}} \hat{\phi}(t)$$
    gegeben. Allerdings gilt
    $$\lim \limits_{k \to \infty} \mu_k \to 0$$
    und wir interessieren uns nur für $T_k(t)$ mit $k \leq M$\\
    Daher schätzen wir $\phi$ durch
    $$\tilde{\phi}_M(t) = \frac{1}{\sqrt{1 - t^2}} \sum_{k = 0}^{M} \mu_k T_k(t)$$
\end{frame}